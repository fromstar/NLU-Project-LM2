{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fromstar/NLU-Project-LM2/blob/main/223727_NLU_LM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykBlgtbm_VB_"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hOI4pbnk_KGq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.optim import optimizer\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWofU_Q_A4N2",
        "outputId": "5d69f13f-4270-4642-ac9e-18f77372a857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "#Main path to dataset:\n",
        "\n",
        "drive.mount('/content/drive/', force_remount = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-g3IesJCBVs1"
      },
      "outputs": [],
      "source": [
        "!cp -R \"/content/drive/MyDrive/input/\"  \"/content/input\" "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global Variables"
      ],
      "metadata": {
        "id": "L6PRXv19wl1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uqQqYUeLEKvb"
      },
      "outputs": [],
      "source": [
        "data = \"/content/input\"\n",
        "device = 'cuda:0'\n",
        "\n",
        "embedding_size = 500\n",
        "hidden_size = 500\n",
        "nlayers = 1\n",
        "learning_rate = 0.001\n",
        "clip = 0.35\n",
        "epochs = 25\n",
        "batch_size = 64\n",
        "eval_batch_size = 1\n",
        "dropout = 0.5\n",
        "interval = 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocCg3Sm2OW_E"
      },
      "source": [
        "Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iHqZXvVtB_aH"
      },
      "outputs": [],
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = {0: '<pad>', 1: '<unk>', 2: '<bos>', 3: '<eos>'}\n",
        "        self.len_dict = len(self.dictionary)\n",
        "        self.train = self.to_token(os.path.join(data, 'ptb.train.txt'))\n",
        "        self.test = self.to_token(os.path.join(data, 'ptb.test.txt'))\n",
        "        self.valid = self.to_token(os.path.join(data, 'ptb.valid.txt'))\n",
        "\n",
        "# Fill the dictionary and return an array with the corresponding key of the words read\n",
        "    def to_token(self, path):\n",
        "        if os.path.exists(path):  # check if the file I need to read exists\n",
        "            with open(path) as txt:\n",
        "                key = self.len_dict\n",
        "                sentences = []\n",
        "                values = list(self.dictionary.values())\n",
        "\n",
        "                for line in txt:\n",
        "                    tmp = []\n",
        "                    # line = line.strip()\n",
        "                    words = ['<bos>'] + line.split() + ['<eos>']\n",
        "\n",
        "                    # scroll through the words of a sentence\n",
        "                    for word in words:\n",
        "                        # if the world is not in the dictionary I add it.\n",
        "                        if word not in values:\n",
        "                            # the length of the dictionary coincides with the index of insertion in it\n",
        "                            self.dictionary[key] = word\n",
        "                            tmp.append(key)\n",
        "                            key += 1\n",
        "                            values.append(word)\n",
        "                        else:\n",
        "                            tmp.append(values.index(word))\n",
        "                    sentences.append(torch.LongTensor(tmp).to(device))\n",
        "\n",
        "            print(\"Sentences loaded\")\n",
        "            self.len_dict = len(self.dictionary)\n",
        "            return sentences\n",
        "        else:\n",
        "            raise ValueError(path +\" doesn't exist.\")\n",
        "\n",
        "    def print_dic(self):\n",
        "        print(self.dictionary)\n",
        "        print(\"Number of tokens: \" + str(len(self.dictionary)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU"
      ],
      "metadata": {
        "id": "iJYOcU8f2KC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "53kUcinzOssO"
      },
      "outputs": [],
      "source": [
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.update_gate = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.reset_gate = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out_gate = nn.Linear(hidden_size, hidden_size)\n",
        "        self.x = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, prev_state):\n",
        "\n",
        "        seq_size, _, _ = input.size()\n",
        "        hidden_seq = []\n",
        "\n",
        "        for t in range(seq_size):\n",
        "\n",
        "            x_t = input[t, :, :]\n",
        "            x_h = torch.cat((x_t, prev_state), dim=1)\n",
        "\n",
        "            reset = torch.sigmoid(self.reset_gate(x_h))\n",
        "            update = torch.sigmoid(self.update_gate(x_h))\n",
        "\n",
        "            n1 = self.out_gate(prev_state) * reset\n",
        "            n2 = n1 + self.x(x_t)\n",
        "            out = torch.tanh(n2)\n",
        "\n",
        "            new_state = (1 - update) * out + update * prev_state\n",
        "            hidden_seq.append(new_state.unsqueeze(0))\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "\n",
        "        return hidden_seq, new_state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "M_QZK2ZT2MWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BR_NLg3_Ov7D"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, ntoken):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.ntoken = ntoken\n",
        "        self.nlayers = nlayers\n",
        "        self.input_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = nn.Embedding(ntoken, self.input_size, padding_idx=0)\n",
        "\n",
        "        self.rnn = nn.ModuleList()\n",
        "        # N GRU layers\n",
        "        for i in range(nlayers):\n",
        "            self.rnn.append(GRUCell(self.input_size, hidden_size))\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size, ntoken)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.05\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.drop(self.encoder(input))\n",
        "\n",
        "        for i in range(len(self.rnn)):\n",
        "            output, hidden[i] = self.rnn[i](output, hidden[i])\n",
        "            output = self.drop(output)\n",
        "\n",
        "        output = self.fc(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1,self.ntoken)\n",
        "\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "\n",
        "        hidden = []\n",
        "        weight = next(self.parameters())\n",
        "        for i in range(nlayers):\n",
        "            hidden.append(weight.new_zeros(batch_size, self.hidden_size))\n",
        "        return hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng3_VxmVIk_U"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n-YU1ZW5tkYP"
      },
      "outputs": [],
      "source": [
        "def get_batch(source, i, batch_size):\n",
        "    data = []\n",
        "    target = []\n",
        "    size = 0\n",
        "    for sentence in source[batch_size * i: batch_size * (i+1)]:\n",
        "        data.append(sentence[:-1])\n",
        "        target.append(sentence[1:])\n",
        "        size += len(sentence[:-1])\n",
        "\n",
        "    # Fill the sentences with the pad tag in order to make them the same length\n",
        "    # The key for the pad tag is 0.\n",
        "    data = pad_sequence(data, padding_value=0)\n",
        "    target = pad_sequence(target, padding_value=0)\n",
        "\n",
        "    return data, target, size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0WxCA7dVtkKy"
      },
      "outputs": [],
      "source": [
        "def train(model, train, opt, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    total_size = 0\n",
        "\n",
        "    for batch_idx in range(0, len(train) // batch_size):\n",
        "        data, target, size = get_batch(train, batch_idx, batch_size)\n",
        "        output, hidden = model(data, hidden)\n",
        "        # Dropout to recurrent element\n",
        "        # output = nn.Dropout(dropout)(output)\n",
        "\n",
        "        loss = F.nll_loss(\n",
        "                output,\n",
        "                target.view(-1),\n",
        "                reduction='sum',\n",
        "                ignore_index=0,\n",
        "            )\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        total_size += size\n",
        "        if batch_idx % interval == 0 and batch_idx > 0:\n",
        "            cur_loss = total_loss / total_size\n",
        "            ppl = round(math.exp(cur_loss),2)\n",
        "            elapsed = time.time() - start_time\n",
        "            print('epoch: ', epoch, ' | batches: ', batch_idx+1, '/', (len(train) // batch_size), ' | learning_rate: ', learning_rate,\n",
        "                  '| ms/batch: ',round(elapsed * 1000 / interval,2), ' | loss: ', round(cur_loss,3), ' | perplexity: ', ppl)\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "            total_size = 0\n",
        "        \n",
        "        for i in range(len(hidden)):\n",
        "            hidden[i] = hidden[i].detach()\n",
        "\n",
        "def evaluate(data_source, model):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_size = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data_source)//eval_batch_size):\n",
        "            hidden = model.init_hidden(eval_batch_size)\n",
        "            data, target, size = get_batch(data_source, i, eval_batch_size)\n",
        "            output, hidden = model(data, hidden)\n",
        "            for i in range(len(hidden)):\n",
        "             hidden[i] = hidden[i].detach()\n",
        "             \n",
        "            total_loss += F.nll_loss(\n",
        "                output,\n",
        "                target.view(-1),\n",
        "                reduction='sum',\n",
        "                ignore_index=0,\n",
        "            )\n",
        "            total_size += size\n",
        "\n",
        "    return total_loss / total_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgZyUehsAw6L",
        "outputId": "2fabf5e2-618f-4d39-891b-9d1475658c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences loaded\n",
            "Sentences loaded\n",
            "Sentences loaded\n",
            "len ditc:  10002\n",
            "Dictionary's length:  10002  words.\n",
            "epoch:  0  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  169.42  | loss:  11.911  | perplexity:  148824.92\n",
            "epoch:  0  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  159.73  | loss:  6.559  | perplexity:  705.22\n",
            "epoch:  0  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.51  | loss:  6.303  | perplexity:  546.48\n",
            "epoch:  0  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.26  | loss:  6.164  | perplexity:  475.36\n",
            "epoch:  0  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.26  | loss:  6.055  | perplexity:  426.33\n",
            "epoch:  0  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  160.89  | loss:  5.955  | perplexity:  385.72\n",
            "epoch:  0  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.21  | loss:  5.889  | perplexity:  361.16\n",
            "epoch:  0  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.89  | loss:  5.804  | perplexity:  331.76\n",
            "epoch:  0  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.51  | loss:  5.808  | perplexity:  332.92\n",
            "epoch:  0  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.52  | loss:  5.828  | perplexity:  339.68\n",
            "epoch:  0  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  161.96  | loss:  5.717  | perplexity:  303.99\n",
            "epoch:  0  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.09  | loss:  5.708  | perplexity:  301.38\n",
            "epoch:  0  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.79  | loss:  5.689  | perplexity:  295.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  0 | time:  144.89 s | valid loss:  6.101 | valid ppl:  446.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  1  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  169.35  | loss:  5.773  | perplexity:  321.51\n",
            "epoch:  1  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.61  | loss:  5.622  | perplexity:  276.47\n",
            "epoch:  1  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  163.75  | loss:  5.636  | perplexity:  280.21\n",
            "epoch:  1  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.55  | loss:  5.618  | perplexity:  275.32\n",
            "epoch:  1  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.58  | loss:  5.601  | perplexity:  270.61\n",
            "epoch:  1  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.57  | loss:  5.558  | perplexity:  259.19\n",
            "epoch:  1  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.15  | loss:  5.538  | perplexity:  254.27\n",
            "epoch:  1  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  155.3  | loss:  5.486  | perplexity:  241.3\n",
            "epoch:  1  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  159.21  | loss:  5.517  | perplexity:  248.94\n",
            "epoch:  1  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.98  | loss:  5.567  | perplexity:  261.58\n",
            "epoch:  1  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  161.63  | loss:  5.468  | perplexity:  237.02\n",
            "epoch:  1  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.76  | loss:  5.476  | perplexity:  238.93\n",
            "epoch:  1  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.35  | loss:  5.467  | perplexity:  236.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  1 | time:  144.1 s | valid loss:  5.486 | valid ppl:  241.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  2  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  169.43  | loss:  5.551  | perplexity:  257.41\n",
            "epoch:  2  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  159.29  | loss:  5.419  | perplexity:  225.75\n",
            "epoch:  2  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.68  | loss:  5.444  | perplexity:  231.47\n",
            "epoch:  2  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.63  | loss:  5.444  | perplexity:  231.25\n",
            "epoch:  2  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.42  | loss:  5.445  | perplexity:  231.6\n",
            "epoch:  2  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.65  | loss:  5.413  | perplexity:  224.19\n",
            "epoch:  2  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  155.36  | loss:  5.405  | perplexity:  222.45\n",
            "epoch:  2  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.92  | loss:  5.349  | perplexity:  210.3\n",
            "epoch:  2  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  159.35  | loss:  5.392  | perplexity:  219.63\n",
            "epoch:  2  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.77  | loss:  5.443  | perplexity:  231.05\n",
            "epoch:  2  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.36  | loss:  5.347  | perplexity:  209.97\n",
            "epoch:  2  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  156.72  | loss:  5.356  | perplexity:  211.8\n",
            "epoch:  2  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.57  | loss:  5.354  | perplexity:  211.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  2 | time:  143.83 s | valid loss:  5.411 | valid ppl:  223.8\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  3  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  167.85  | loss:  5.433  | perplexity:  228.9\n",
            "epoch:  3  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.48  | loss:  5.327  | perplexity:  205.74\n",
            "epoch:  3  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.59  | loss:  5.36  | perplexity:  212.66\n",
            "epoch:  3  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  166.77  | loss:  5.341  | perplexity:  208.73\n",
            "epoch:  3  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  154.89  | loss:  5.356  | perplexity:  211.92\n",
            "epoch:  3  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.04  | loss:  5.318  | perplexity:  203.9\n",
            "epoch:  3  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.56  | loss:  5.318  | perplexity:  204.01\n",
            "epoch:  3  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.38  | loss:  5.261  | perplexity:  192.72\n",
            "epoch:  3  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.43  | loss:  5.309  | perplexity:  202.15\n",
            "epoch:  3  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.86  | loss:  5.363  | perplexity:  213.43\n",
            "epoch:  3  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  161.69  | loss:  5.268  | perplexity:  193.95\n",
            "epoch:  3  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  158.23  | loss:  5.279  | perplexity:  196.21\n",
            "epoch:  3  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.57  | loss:  5.279  | perplexity:  196.1\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  3 | time:  143.8 s | valid loss:  5.383 | valid ppl:  217.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  4  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.44  | loss:  5.353  | perplexity:  211.32\n",
            "epoch:  4  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  157.72  | loss:  5.255  | perplexity:  191.49\n",
            "epoch:  4  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.76  | loss:  5.284  | perplexity:  197.09\n",
            "epoch:  4  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  166.52  | loss:  5.287  | perplexity:  197.73\n",
            "epoch:  4  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  154.81  | loss:  5.289  | perplexity:  198.23\n",
            "epoch:  4  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  160.78  | loss:  5.254  | perplexity:  191.29\n",
            "epoch:  4  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.63  | loss:  5.258  | perplexity:  192.05\n",
            "epoch:  4  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.67  | loss:  5.202  | perplexity:  181.71\n",
            "epoch:  4  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.9  | loss:  5.254  | perplexity:  191.27\n",
            "epoch:  4  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.42  | loss:  5.305  | perplexity:  201.25\n",
            "epoch:  4  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  161.4  | loss:  5.212  | perplexity:  183.45\n",
            "epoch:  4  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.04  | loss:  5.221  | perplexity:  185.03\n",
            "epoch:  4  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.77  | loss:  5.226  | perplexity:  186.0\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  4 | time:  144.27 s | valid loss:  5.37 | valid ppl:  214.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  5  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.46  | loss:  5.302  | perplexity:  200.79\n",
            "epoch:  5  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.69  | loss:  5.211  | perplexity:  183.36\n",
            "epoch:  5  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.85  | loss:  5.231  | perplexity:  187.01\n",
            "epoch:  5  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  166.73  | loss:  5.235  | perplexity:  187.73\n",
            "epoch:  5  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  154.8  | loss:  5.243  | perplexity:  189.27\n",
            "epoch:  5  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.01  | loss:  5.211  | perplexity:  183.3\n",
            "epoch:  5  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.65  | loss:  5.211  | perplexity:  183.31\n",
            "epoch:  5  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  155.1  | loss:  5.156  | perplexity:  173.53\n",
            "epoch:  5  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  159.13  | loss:  5.204  | perplexity:  181.91\n",
            "epoch:  5  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.48  | loss:  5.257  | perplexity:  191.85\n",
            "epoch:  5  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.2  | loss:  5.168  | perplexity:  175.62\n",
            "epoch:  5  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  158.01  | loss:  5.175  | perplexity:  176.86\n",
            "epoch:  5  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.8  | loss:  5.183  | perplexity:  178.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  5 | time:  144.24 s | valid loss:  5.371 | valid ppl:  215.0\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  6  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.0  | loss:  5.262  | perplexity:  192.91\n",
            "epoch:  6  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.89  | loss:  5.174  | perplexity:  176.54\n",
            "epoch:  6  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.37  | loss:  5.194  | perplexity:  180.18\n",
            "epoch:  6  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.71  | loss:  5.195  | perplexity:  180.43\n",
            "epoch:  6  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.44  | loss:  5.205  | perplexity:  182.22\n",
            "epoch:  6  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  160.81  | loss:  5.177  | perplexity:  177.2\n",
            "epoch:  6  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  153.8  | loss:  5.171  | perplexity:  176.01\n",
            "epoch:  6  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.72  | loss:  5.125  | perplexity:  168.26\n",
            "epoch:  6  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.29  | loss:  5.168  | perplexity:  175.57\n",
            "epoch:  6  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.72  | loss:  5.223  | perplexity:  185.42\n",
            "epoch:  6  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.04  | loss:  5.134  | perplexity:  169.68\n",
            "epoch:  6  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.24  | loss:  5.142  | perplexity:  171.02\n",
            "epoch:  6  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.71  | loss:  5.153  | perplexity:  172.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  6 | time:  144.08 s | valid loss:  5.367 | valid ppl:  214.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  7  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  167.52  | loss:  5.225  | perplexity:  185.94\n",
            "epoch:  7  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.99  | loss:  5.138  | perplexity:  170.32\n",
            "epoch:  7  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.53  | loss:  5.172  | perplexity:  176.26\n",
            "epoch:  7  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.57  | loss:  5.164  | perplexity:  174.78\n",
            "epoch:  7  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.31  | loss:  5.18  | perplexity:  177.63\n",
            "epoch:  7  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  160.0  | loss:  5.149  | perplexity:  172.29\n",
            "epoch:  7  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.72  | loss:  5.141  | perplexity:  170.84\n",
            "epoch:  7  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.91  | loss:  5.097  | perplexity:  163.61\n",
            "epoch:  7  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.91  | loss:  5.135  | perplexity:  169.94\n",
            "epoch:  7  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.74  | loss:  5.203  | perplexity:  181.78\n",
            "epoch:  7  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.31  | loss:  5.098  | perplexity:  163.71\n",
            "epoch:  7  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.42  | loss:  5.11  | perplexity:  165.74\n",
            "epoch:  7  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.43  | loss:  5.125  | perplexity:  168.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  7 | time:  143.51 s | valid loss:  5.371 | valid ppl:  214.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  8  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.36  | loss:  5.195  | perplexity:  180.43\n",
            "epoch:  8  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.21  | loss:  5.119  | perplexity:  167.14\n",
            "epoch:  8  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.35  | loss:  5.139  | perplexity:  170.61\n",
            "epoch:  8  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.01  | loss:  5.144  | perplexity:  171.45\n",
            "epoch:  8  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.07  | loss:  5.153  | perplexity:  172.97\n",
            "epoch:  8  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.27  | loss:  5.117  | perplexity:  166.89\n",
            "epoch:  8  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.59  | loss:  5.112  | perplexity:  166.02\n",
            "epoch:  8  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.78  | loss:  5.064  | perplexity:  158.18\n",
            "epoch:  8  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.57  | loss:  5.112  | perplexity:  166.04\n",
            "epoch:  8  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.15  | loss:  5.173  | perplexity:  176.49\n",
            "epoch:  8  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.31  | loss:  5.074  | perplexity:  159.79\n",
            "epoch:  8  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  156.92  | loss:  5.091  | perplexity:  162.51\n",
            "epoch:  8  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.38  | loss:  5.09  | perplexity:  162.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  8 | time:  143.88 s | valid loss:  5.375 | valid ppl:  216.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  9  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  167.82  | loss:  5.176  | perplexity:  176.99\n",
            "epoch:  9  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.65  | loss:  5.094  | perplexity:  163.04\n",
            "epoch:  9  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  163.01  | loss:  5.113  | perplexity:  166.14\n",
            "epoch:  9  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.13  | loss:  5.119  | perplexity:  167.17\n",
            "epoch:  9  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  154.87  | loss:  5.13  | perplexity:  168.96\n",
            "epoch:  9  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  160.46  | loss:  5.095  | perplexity:  163.14\n",
            "epoch:  9  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.27  | loss:  5.091  | perplexity:  162.57\n",
            "epoch:  9  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.81  | loss:  5.044  | perplexity:  155.03\n",
            "epoch:  9  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.81  | loss:  5.093  | perplexity:  162.86\n",
            "epoch:  9  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.4  | loss:  5.148  | perplexity:  172.14\n",
            "epoch:  9  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  161.93  | loss:  5.046  | perplexity:  155.36\n",
            "epoch:  9  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.27  | loss:  5.069  | perplexity:  159.06\n",
            "epoch:  9  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.95  | loss:  5.067  | perplexity:  158.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  9 | time:  143.75 s | valid loss:  5.383 | valid ppl:  217.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  10  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  169.04  | loss:  5.153  | perplexity:  172.87\n",
            "epoch:  10  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  157.93  | loss:  5.077  | perplexity:  160.35\n",
            "epoch:  10  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.73  | loss:  5.093  | perplexity:  162.83\n",
            "epoch:  10  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  166.87  | loss:  5.097  | perplexity:  163.48\n",
            "epoch:  10  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  154.65  | loss:  5.107  | perplexity:  165.21\n",
            "epoch:  10  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.21  | loss:  5.073  | perplexity:  159.66\n",
            "epoch:  10  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  153.97  | loss:  5.07  | perplexity:  159.14\n",
            "epoch:  10  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.75  | loss:  5.025  | perplexity:  152.11\n",
            "epoch:  10  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.96  | loss:  5.07  | perplexity:  159.23\n",
            "epoch:  10  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.36  | loss:  5.122  | perplexity:  167.65\n",
            "epoch:  10  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  161.51  | loss:  5.031  | perplexity:  153.05\n",
            "epoch:  10  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.47  | loss:  5.048  | perplexity:  155.78\n",
            "epoch:  10  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.06  | loss:  5.049  | perplexity:  155.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  10 | time:  143.37 s | valid loss:  5.399 | valid ppl:  221.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  11  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  169.12  | loss:  5.138  | perplexity:  170.43\n",
            "epoch:  11  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  159.23  | loss:  5.056  | perplexity:  157.02\n",
            "epoch:  11  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.19  | loss:  5.068  | perplexity:  158.78\n",
            "epoch:  11  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.63  | loss:  5.083  | perplexity:  161.28\n",
            "epoch:  11  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.36  | loss:  5.096  | perplexity:  163.43\n",
            "epoch:  11  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.19  | loss:  5.051  | perplexity:  156.13\n",
            "epoch:  11  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  153.98  | loss:  5.049  | perplexity:  155.89\n",
            "epoch:  11  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.21  | loss:  5.0  | perplexity:  148.38\n",
            "epoch:  11  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.24  | loss:  5.045  | perplexity:  155.23\n",
            "epoch:  11  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.44  | loss:  5.112  | perplexity:  166.07\n",
            "epoch:  11  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.29  | loss:  5.012  | perplexity:  150.27\n",
            "epoch:  11  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.2  | loss:  5.034  | perplexity:  153.59\n",
            "epoch:  11  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.77  | loss:  5.028  | perplexity:  152.7\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  11 | time:  143.55 s | valid loss:  5.403 | valid ppl:  222.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  12  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.91  | loss:  5.121  | perplexity:  167.56\n",
            "epoch:  12  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.35  | loss:  5.04  | perplexity:  154.52\n",
            "epoch:  12  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  163.28  | loss:  5.058  | perplexity:  157.27\n",
            "epoch:  12  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  166.93  | loss:  5.066  | perplexity:  158.58\n",
            "epoch:  12  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.35  | loss:  5.078  | perplexity:  160.41\n",
            "epoch:  12  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  160.2  | loss:  5.029  | perplexity:  152.75\n",
            "epoch:  12  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.35  | loss:  5.034  | perplexity:  153.5\n",
            "epoch:  12  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.78  | loss:  4.987  | perplexity:  146.49\n",
            "epoch:  12  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.93  | loss:  5.03  | perplexity:  152.92\n",
            "epoch:  12  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.4  | loss:  5.092  | perplexity:  162.71\n",
            "epoch:  12  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.14  | loss:  4.99  | perplexity:  146.89\n",
            "epoch:  12  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  158.27  | loss:  5.018  | perplexity:  151.16\n",
            "epoch:  12  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.09  | loss:  5.01  | perplexity:  149.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  12 | time:  143.72 s | valid loss:  5.403 | valid ppl:  221.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  13  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.41  | loss:  5.099  | perplexity:  163.87\n",
            "epoch:  13  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.62  | loss:  5.02  | perplexity:  151.49\n",
            "epoch:  13  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.88  | loss:  5.044  | perplexity:  155.11\n",
            "epoch:  13  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.16  | loss:  5.047  | perplexity:  155.59\n",
            "epoch:  13  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  154.99  | loss:  5.069  | perplexity:  159.03\n",
            "epoch:  13  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  160.38  | loss:  5.021  | perplexity:  151.52\n",
            "epoch:  13  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.6  | loss:  5.017  | perplexity:  150.92\n",
            "epoch:  13  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.56  | loss:  4.968  | perplexity:  143.74\n",
            "epoch:  13  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.95  | loss:  5.021  | perplexity:  151.56\n",
            "epoch:  13  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.14  | loss:  5.075  | perplexity:  160.01\n",
            "epoch:  13  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  161.77  | loss:  4.97  | perplexity:  143.98\n",
            "epoch:  13  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.28  | loss:  5.004  | perplexity:  149.05\n",
            "epoch:  13  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.25  | loss:  5.0  | perplexity:  148.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  13 | time:  144.79 s | valid loss:  5.417 | valid ppl:  225.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  14  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.77  | loss:  5.092  | perplexity:  162.78\n",
            "epoch:  14  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.94  | loss:  5.009  | perplexity:  149.69\n",
            "epoch:  14  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  163.46  | loss:  5.03  | perplexity:  153.01\n",
            "epoch:  14  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.48  | loss:  5.038  | perplexity:  154.13\n",
            "epoch:  14  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.31  | loss:  5.06  | perplexity:  157.54\n",
            "epoch:  14  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.73  | loss:  5.005  | perplexity:  149.09\n",
            "epoch:  14  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.99  | loss:  5.001  | perplexity:  148.59\n",
            "epoch:  14  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.76  | loss:  4.956  | perplexity:  142.04\n",
            "epoch:  14  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  159.43  | loss:  5.006  | perplexity:  149.36\n",
            "epoch:  14  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.86  | loss:  5.062  | perplexity:  157.98\n",
            "epoch:  14  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  161.54  | loss:  4.962  | perplexity:  142.82\n",
            "epoch:  14  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  158.17  | loss:  4.991  | perplexity:  147.05\n",
            "epoch:  14  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.11  | loss:  4.988  | perplexity:  146.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  14 | time:  145.48 s | valid loss:  5.427 | valid ppl:  227.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  15  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  170.0  | loss:  5.081  | perplexity:  161.0\n",
            "epoch:  15  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  159.62  | loss:  4.993  | perplexity:  147.45\n",
            "epoch:  15  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  163.87  | loss:  5.025  | perplexity:  152.16\n",
            "epoch:  15  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  168.22  | loss:  5.027  | perplexity:  152.47\n",
            "epoch:  15  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.25  | loss:  5.042  | perplexity:  154.7\n",
            "epoch:  15  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  160.27  | loss:  4.994  | perplexity:  147.56\n",
            "epoch:  15  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.24  | loss:  4.987  | perplexity:  146.49\n",
            "epoch:  15  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.78  | loss:  4.938  | perplexity:  139.54\n",
            "epoch:  15  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.6  | loss:  4.995  | perplexity:  147.68\n",
            "epoch:  15  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.57  | loss:  5.048  | perplexity:  155.69\n",
            "epoch:  15  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  161.74  | loss:  4.951  | perplexity:  141.28\n",
            "epoch:  15  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.99  | loss:  4.978  | perplexity:  145.14\n",
            "epoch:  15  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.15  | loss:  4.974  | perplexity:  144.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  15 | time:  144.24 s | valid loss:  5.433 | valid ppl:  228.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  16  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  167.94  | loss:  5.064  | perplexity:  158.25\n",
            "epoch:  16  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.35  | loss:  4.989  | perplexity:  146.73\n",
            "epoch:  16  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.69  | loss:  5.005  | perplexity:  149.1\n",
            "epoch:  16  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.78  | loss:  5.015  | perplexity:  150.58\n",
            "epoch:  16  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  154.79  | loss:  5.027  | perplexity:  152.49\n",
            "epoch:  16  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  160.76  | loss:  4.983  | perplexity:  145.88\n",
            "epoch:  16  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.7  | loss:  4.977  | perplexity:  145.0\n",
            "epoch:  16  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.92  | loss:  4.936  | perplexity:  139.21\n",
            "epoch:  16  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.11  | loss:  4.977  | perplexity:  145.04\n",
            "epoch:  16  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.39  | loss:  5.041  | perplexity:  154.67\n",
            "epoch:  16  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.12  | loss:  4.94  | perplexity:  139.76\n",
            "epoch:  16  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  158.01  | loss:  4.964  | perplexity:  143.13\n",
            "epoch:  16  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.78  | loss:  4.969  | perplexity:  143.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  16 | time:  143.87 s | valid loss:  5.436 | valid ppl:  229.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  17  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  167.95  | loss:  5.049  | perplexity:  155.91\n",
            "epoch:  17  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.72  | loss:  4.975  | perplexity:  144.69\n",
            "epoch:  17  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.97  | loss:  5.004  | perplexity:  148.96\n",
            "epoch:  17  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  168.58  | loss:  5.002  | perplexity:  148.76\n",
            "epoch:  17  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.25  | loss:  5.025  | perplexity:  152.18\n",
            "epoch:  17  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.12  | loss:  4.977  | perplexity:  145.03\n",
            "epoch:  17  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  155.17  | loss:  4.967  | perplexity:  143.57\n",
            "epoch:  17  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.87  | loss:  4.928  | perplexity:  138.08\n",
            "epoch:  17  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.89  | loss:  4.968  | perplexity:  143.71\n",
            "epoch:  17  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.21  | loss:  5.028  | perplexity:  152.62\n",
            "epoch:  17  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.28  | loss:  4.933  | perplexity:  138.82\n",
            "epoch:  17  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.26  | loss:  4.952  | perplexity:  141.42\n",
            "epoch:  17  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.81  | loss:  4.955  | perplexity:  141.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  17 | time:  144.31 s | valid loss:  5.454 | valid ppl:  233.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  18  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.61  | loss:  5.04  | perplexity:  154.49\n",
            "epoch:  18  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.66  | loss:  4.966  | perplexity:  143.48\n",
            "epoch:  18  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.1  | loss:  4.982  | perplexity:  145.83\n",
            "epoch:  18  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.67  | loss:  4.993  | perplexity:  147.36\n",
            "epoch:  18  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.36  | loss:  5.018  | perplexity:  151.18\n",
            "epoch:  18  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.34  | loss:  4.969  | perplexity:  143.95\n",
            "epoch:  18  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.91  | loss:  4.956  | perplexity:  142.03\n",
            "epoch:  18  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.49  | loss:  4.917  | perplexity:  136.59\n",
            "epoch:  18  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.3  | loss:  4.955  | perplexity:  141.86\n",
            "epoch:  18  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.2  | loss:  5.021  | perplexity:  151.5\n",
            "epoch:  18  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.18  | loss:  4.917  | perplexity:  136.65\n",
            "epoch:  18  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.5  | loss:  4.948  | perplexity:  140.96\n",
            "epoch:  18  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  161.92  | loss:  4.952  | perplexity:  141.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  18 | time:  144.93 s | valid loss:  5.457 | valid ppl:  234.5\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  19  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  167.71  | loss:  5.032  | perplexity:  153.29\n",
            "epoch:  19  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.21  | loss:  4.958  | perplexity:  142.26\n",
            "epoch:  19  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  163.02  | loss:  4.981  | perplexity:  145.61\n",
            "epoch:  19  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  166.65  | loss:  4.985  | perplexity:  146.18\n",
            "epoch:  19  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.08  | loss:  5.007  | perplexity:  149.4\n",
            "epoch:  19  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  160.62  | loss:  4.952  | perplexity:  141.47\n",
            "epoch:  19  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.04  | loss:  4.953  | perplexity:  141.6\n",
            "epoch:  19  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.67  | loss:  4.904  | perplexity:  134.86\n",
            "epoch:  19  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.65  | loss:  4.952  | perplexity:  141.51\n",
            "epoch:  19  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.15  | loss:  5.012  | perplexity:  150.26\n",
            "epoch:  19  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.24  | loss:  4.901  | perplexity:  134.38\n",
            "epoch:  19  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.85  | loss:  4.942  | perplexity:  140.02\n",
            "epoch:  19  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.09  | loss:  4.943  | perplexity:  140.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  19 | time:  144.62 s | valid loss:  5.467 | valid ppl:  236.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  20  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.72  | loss:  5.023  | perplexity:  151.85\n",
            "epoch:  20  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.25  | loss:  4.945  | perplexity:  140.5\n",
            "epoch:  20  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.73  | loss:  4.968  | perplexity:  143.67\n",
            "epoch:  20  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  166.9  | loss:  4.979  | perplexity:  145.26\n",
            "epoch:  20  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.5  | loss:  4.998  | perplexity:  148.05\n",
            "epoch:  20  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.21  | loss:  4.939  | perplexity:  139.6\n",
            "epoch:  20  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.6  | loss:  4.952  | perplexity:  141.4\n",
            "epoch:  20  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.37  | loss:  4.899  | perplexity:  134.13\n",
            "epoch:  20  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.18  | loss:  4.943  | perplexity:  140.26\n",
            "epoch:  20  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.32  | loss:  5.004  | perplexity:  148.99\n",
            "epoch:  20  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  163.43  | loss:  4.9  | perplexity:  134.23\n",
            "epoch:  20  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.57  | loss:  4.931  | perplexity:  138.51\n",
            "epoch:  20  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.93  | loss:  4.926  | perplexity:  137.8\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  20 | time:  146.07 s | valid loss:  5.482 | valid ppl:  240.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  21  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.89  | loss:  5.01  | perplexity:  149.94\n",
            "epoch:  21  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.17  | loss:  4.945  | perplexity:  140.53\n",
            "epoch:  21  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  163.55  | loss:  4.952  | perplexity:  141.51\n",
            "epoch:  21  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.76  | loss:  4.974  | perplexity:  144.62\n",
            "epoch:  21  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  154.8  | loss:  4.996  | perplexity:  147.77\n",
            "epoch:  21  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.77  | loss:  4.929  | perplexity:  138.28\n",
            "epoch:  21  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  155.1  | loss:  4.946  | perplexity:  140.61\n",
            "epoch:  21  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.78  | loss:  4.89  | perplexity:  132.95\n",
            "epoch:  21  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  158.45  | loss:  4.931  | perplexity:  138.48\n",
            "epoch:  21  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  157.66  | loss:  4.988  | perplexity:  146.72\n",
            "epoch:  21  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.38  | loss:  4.888  | perplexity:  132.72\n",
            "epoch:  21  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  158.56  | loss:  4.927  | perplexity:  137.92\n",
            "epoch:  21  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.24  | loss:  4.922  | perplexity:  137.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  21 | time:  145.36 s | valid loss:  5.495 | valid ppl:  243.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  22  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  167.36  | loss:  5.011  | perplexity:  149.99\n",
            "epoch:  22  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  159.09  | loss:  4.939  | perplexity:  139.62\n",
            "epoch:  22  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.73  | loss:  4.954  | perplexity:  141.69\n",
            "epoch:  22  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  167.18  | loss:  4.969  | perplexity:  143.94\n",
            "epoch:  22  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.54  | loss:  4.984  | perplexity:  146.07\n",
            "epoch:  22  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.19  | loss:  4.926  | perplexity:  137.82\n",
            "epoch:  22  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  154.25  | loss:  4.929  | perplexity:  138.18\n",
            "epoch:  22  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  155.21  | loss:  4.895  | perplexity:  133.56\n",
            "epoch:  22  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  159.35  | loss:  4.93  | perplexity:  138.33\n",
            "epoch:  22  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.75  | loss:  5.0  | perplexity:  148.46\n",
            "epoch:  22  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  163.01  | loss:  4.879  | perplexity:  131.47\n",
            "epoch:  22  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.82  | loss:  4.921  | perplexity:  137.16\n",
            "epoch:  22  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.94  | loss:  4.914  | perplexity:  136.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  22 | time:  145.39 s | valid loss:  5.491 | valid ppl:  242.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  23  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  169.26  | loss:  4.994  | perplexity:  147.6\n",
            "epoch:  23  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  158.75  | loss:  4.93  | perplexity:  138.37\n",
            "epoch:  23  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  162.89  | loss:  4.943  | perplexity:  140.14\n",
            "epoch:  23  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  168.42  | loss:  4.963  | perplexity:  143.04\n",
            "epoch:  23  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.38  | loss:  4.979  | perplexity:  145.32\n",
            "epoch:  23  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  162.25  | loss:  4.923  | perplexity:  137.4\n",
            "epoch:  23  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  155.28  | loss:  4.923  | perplexity:  137.42\n",
            "epoch:  23  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  154.88  | loss:  4.884  | perplexity:  132.18\n",
            "epoch:  23  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  159.81  | loss:  4.91  | perplexity:  135.6\n",
            "epoch:  23  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.37  | loss:  4.981  | perplexity:  145.57\n",
            "epoch:  23  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  161.84  | loss:  4.879  | perplexity:  131.44\n",
            "epoch:  23  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.78  | loss:  4.909  | perplexity:  135.48\n",
            "epoch:  23  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  163.34  | loss:  4.909  | perplexity:  135.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  23 | time:  146.63 s | valid loss:  5.509 | valid ppl:  246.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "epoch:  24  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  168.2  | loss:  4.988  | perplexity:  146.59\n",
            "epoch:  24  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  159.53  | loss:  4.935  | perplexity:  139.04\n",
            "epoch:  24  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  163.47  | loss:  4.93  | perplexity:  138.38\n",
            "epoch:  24  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  168.02  | loss:  4.956  | perplexity:  142.05\n",
            "epoch:  24  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  155.88  | loss:  4.973  | perplexity:  144.48\n",
            "epoch:  24  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  161.29  | loss:  4.915  | perplexity:  136.29\n",
            "epoch:  24  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  155.16  | loss:  4.911  | perplexity:  135.78\n",
            "epoch:  24  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  155.72  | loss:  4.879  | perplexity:  131.51\n",
            "epoch:  24  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  159.27  | loss:  4.909  | perplexity:  135.53\n",
            "epoch:  24  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  158.9  | loss:  4.979  | perplexity:  145.33\n",
            "epoch:  24  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  162.05  | loss:  4.87  | perplexity:  130.27\n",
            "epoch:  24  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  157.91  | loss:  4.896  | perplexity:  133.8\n",
            "epoch:  24  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  162.04  | loss:  4.908  | perplexity:  135.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "end epoch:  24 | time:  145.82 s | valid loss:  5.495 | valid ppl:  243.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "-----------------------------------------------------------------------------------------\n",
            "End of training\n",
            "test loss:  5.303 \n",
            "test ppl:  200.9382383822465\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    save = 'model_test.pt'\n",
        "    torch.manual_seed(1111)\n",
        "    corpus = Corpus()\n",
        "    print(\"len ditc: \", corpus.len_dict)\n",
        "    model = RNN(corpus.len_dict).to(device)\n",
        "    print(\"Dictionary's length: \", corpus.len_dict, \" words.\")\n",
        "\n",
        "    train_data = corpus.train\n",
        "    test_data = corpus.test\n",
        "    val_data = corpus.valid\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99))\n",
        "    # opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    best_val_loss = None\n",
        "\n",
        "    try:\n",
        "        for epoch in range(0, epochs):\n",
        "            epoch_start_time = time.time()\n",
        "            train(model, train_data, opt, epoch)\n",
        "            val_loss = evaluate(val_data, model)\n",
        "            ppl = round(math.exp(val_loss),2)\n",
        "\n",
        "            print(\"-----------------------------------------------------------------------------------------\")\n",
        "            print('end epoch: ', epoch, '| time: ', round((time.time() - epoch_start_time),2), 's | valid loss: ', round(val_loss.item(),3),\n",
        "                  '| valid ppl: ', ppl)\n",
        "            print(\"-----------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "            if not best_val_loss or val_loss < best_val_loss:\n",
        "                with open(save, 'wb') as f:\n",
        "                    torch.save(model, f)\n",
        "                best_val_loss = val_loss\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting from training early')\n",
        "\n",
        "    with open(save, 'rb') as f:\n",
        "        model = torch.load(f)\n",
        "\n",
        "    test_loss = evaluate(test_data,model)\n",
        "    print(\"-----------------------------------------------------------------------------------------\")\n",
        "    print('End of training\\ntest loss: ', round(test_loss.item(),3), '\\ntest ppl: ', math.exp(test_loss))\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jVAJ9T-wCqEd"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "223727_NLU_LM2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}