{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fromstar/NLU-Project-LM2/blob/main/223727_NLU_LM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykBlgtbm_VB_"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOI4pbnk_KGq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.optim import optimizer\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWofU_Q_A4N2",
        "outputId": "bc016c3f-8b0a-46c0-9463-0a0a1b817248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "#Main path to dataset:\n",
        "\n",
        "drive.mount('/content/drive/', force_remount = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g3IesJCBVs1"
      },
      "outputs": [],
      "source": [
        "!cp -R \"/content/drive/MyDrive/input/\"  \"/content/input\" "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global Variables"
      ],
      "metadata": {
        "id": "L6PRXv19wl1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqQqYUeLEKvb"
      },
      "outputs": [],
      "source": [
        "data = \"/content/input\"\n",
        "device = 'cuda:0'\n",
        "\n",
        "embedding_size = 512\n",
        "hidden_size = 512\n",
        "nlayers = 1\n",
        "learning_rate = 0.001\n",
        "clip = 0.35\n",
        "epochs = 25\n",
        "batch_size = 64\n",
        "eval_batch_size = 1\n",
        "dropout = 0.5\n",
        "interval = 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocCg3Sm2OW_E"
      },
      "source": [
        "Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHqZXvVtB_aH"
      },
      "outputs": [],
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = {0: '<pad>', 1: '<unk>', 2: '<bos>', 3: '<eos>'}\n",
        "        self.len_dict = len(self.dictionary)\n",
        "        self.train = self.to_token(os.path.join(data, 'ptb.train.txt'))\n",
        "        self.test = self.to_token(os.path.join(data, 'ptb.test.txt'))\n",
        "        self.valid = self.to_token(os.path.join(data, 'ptb.valid.txt'))\n",
        "\n",
        "# Fill the dictionary and return an array with the corresponding key of the words read\n",
        "    def to_token(self, path):\n",
        "        if os.path.exists(path):  # check if the file I need to read exists\n",
        "            with open(path) as txt:\n",
        "                key = self.len_dict\n",
        "                sentences = []\n",
        "                values = list(self.dictionary.values())\n",
        "\n",
        "                for line in txt:\n",
        "                    tmp = []\n",
        "                    # line = line.strip()\n",
        "                    words = ['<bos>'] + line.split() + ['<eos>']\n",
        "\n",
        "                    # scroll through the words of a sentence\n",
        "                    for word in words:\n",
        "                        # if the world is not in the dictionary I add it.\n",
        "                        if word not in values:\n",
        "                            # the length of the dictionary coincides with the index of insertion in it\n",
        "                            self.dictionary[key] = word\n",
        "                            tmp.append(key)\n",
        "                            key += 1\n",
        "                            values.append(word)\n",
        "                        else:\n",
        "                            tmp.append(values.index(word))\n",
        "                    sentences.append(torch.LongTensor(tmp).to(device))\n",
        "\n",
        "            print(\"Sentences loaded\")\n",
        "            self.len_dict = len(self.dictionary)\n",
        "            return sentences\n",
        "        else:\n",
        "            raise ValueError(path +\" doesn't exist.\")\n",
        "\n",
        "    def print_dic(self):\n",
        "        print(self.dictionary)\n",
        "        print(\"Number of tokens: \" + str(len(self.dictionary)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU"
      ],
      "metadata": {
        "id": "iJYOcU8f2KC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53kUcinzOssO"
      },
      "outputs": [],
      "source": [
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.update_gate = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.reset_gate = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out_gate = nn.Linear(hidden_size, hidden_size)\n",
        "        self.x = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, prev_state):\n",
        "\n",
        "        seq_size, _, _ = input.size()\n",
        "        hidden_seq = []\n",
        "\n",
        "        for t in range(seq_size):\n",
        "\n",
        "            x_t = input[t, :, :]\n",
        "            x_h = torch.cat((x_t, prev_state), dim=1)\n",
        "\n",
        "            reset = torch.sigmoid(self.reset_gate(x_h))\n",
        "            update = torch.sigmoid(self.update_gate(x_h))\n",
        "\n",
        "            n1 = self.out_gate(prev_state) * reset\n",
        "            n2 = n1 + self.x(x_t)\n",
        "            out = torch.tanh(n2)\n",
        "\n",
        "            new_state = (1 - update) * out + update * prev_state\n",
        "            hidden_seq.append(new_state.unsqueeze(0))\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "\n",
        "        return hidden_seq, new_state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "M_QZK2ZT2MWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR_NLg3_Ov7D"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, ntoken):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.ntoken = ntoken\n",
        "        self.nlayers = nlayers\n",
        "        self.input_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = nn.Embedding(ntoken, self.input_size, padding_idx=0)\n",
        "\n",
        "        self.rnn = nn.ModuleList()\n",
        "        # N GRU layers\n",
        "        for i in range(nlayers):\n",
        "            self.rnn.append(GRUCell(self.input_size, hidden_size))\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size, ntoken)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.05\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.drop(self.encoder(input))\n",
        "\n",
        "        for i in range(len(self.rnn)):\n",
        "            output, hidden[i] = self.rnn[i](output, hidden[i])\n",
        "            output = self.drop(output)\n",
        "\n",
        "        output = self.fc(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1,self.ntoken)\n",
        "\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "\n",
        "        hidden = []\n",
        "        weight = next(self.parameters())\n",
        "        for i in range(nlayers):\n",
        "            hidden.append(weight.new_zeros(batch_size, self.hidden_size))\n",
        "        return hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng3_VxmVIk_U"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-YU1ZW5tkYP"
      },
      "outputs": [],
      "source": [
        "def get_batch(source, i, batch_size):\n",
        "    data = []\n",
        "    target = []\n",
        "    size = 0\n",
        "    for sentence in source[batch_size * i: batch_size * (i+1)]:\n",
        "        data.append(sentence[:-1])\n",
        "        target.append(sentence[1:])\n",
        "        size += len(sentence[:-1])\n",
        "\n",
        "    # Fill the sentences with the pad tag in order to make them the same length\n",
        "    # The key for the pad tag is 0.\n",
        "    data = pad_sequence(data, padding_value=0)\n",
        "    target = pad_sequence(target, padding_value=0)\n",
        "\n",
        "    return data, target, size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WxCA7dVtkKy"
      },
      "outputs": [],
      "source": [
        "def train(model, train, opt, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    total_size = 0\n",
        "\n",
        "    for batch_idx in range(0, len(train) // batch_size):\n",
        "        data, target, size = get_batch(train, batch_idx, batch_size)\n",
        "        output, hidden = model(data, hidden)\n",
        "        # Dropout to recurrent element\n",
        "        # output = nn.Dropout(dropout)(output)\n",
        "\n",
        "        loss = F.nll_loss(\n",
        "                output,\n",
        "                target.view(-1),\n",
        "                reduction='sum',\n",
        "                ignore_index=0,\n",
        "            )\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        total_size += size\n",
        "        if batch_idx % interval == 0 and batch_idx > 0:\n",
        "            cur_loss = total_loss / total_size\n",
        "            ppl = round(math.exp(cur_loss),2)\n",
        "            elapsed = time.time() - start_time\n",
        "            print('epoch: ', epoch, ' | batches: ', batch_idx+1, '/', (len(train) // batch_size), ' | learning_rate: ', learning_rate,\n",
        "                  '| ms/batch: ',round(elapsed * 1000 / interval,2), ' | loss: ', round(cur_loss,3), ' | perplexity: ', ppl)\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "            total_size = 0\n",
        "        \n",
        "        for i in range(len(hidden)):\n",
        "            hidden[i] = hidden[i].detach()\n",
        "\n",
        "def evaluate(data_source, model):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_size = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(data_source)//eval_batch_size):\n",
        "            hidden = model.init_hidden(eval_batch_size)\n",
        "            data, target, size = get_batch(data_source, i, eval_batch_size)\n",
        "            output, hidden = model(data, hidden)\n",
        "            for i in range(len(hidden)):\n",
        "             hidden[i] = hidden[i].detach()\n",
        "             \n",
        "            total_loss += F.nll_loss(\n",
        "                output,\n",
        "                target.view(-1),\n",
        "                reduction='sum',\n",
        "                ignore_index=0,\n",
        "            )\n",
        "            total_size += size\n",
        "\n",
        "    return total_loss / total_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FgZyUehsAw6L",
        "outputId": "866199e8-edcd-49ba-842b-a5925aaeab60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences loaded\n",
            "Sentences loaded\n",
            "Sentences loaded\n",
            "len ditc:  10002\n",
            "Dictionary's length:  10002  words.\n",
            "epoch:  0  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  234.0  | loss:  9.888  | perplexity:  19685.43\n",
            "epoch:  0  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  219.24  | loss:  6.634  | perplexity:  760.6\n",
            "epoch:  0  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  225.56  | loss:  6.41  | perplexity:  608.05\n",
            "epoch:  0  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  231.38  | loss:  6.284  | perplexity:  535.96\n",
            "epoch:  0  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  214.49  | loss:  6.185  | perplexity:  485.6\n",
            "epoch:  0  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  223.41  | loss:  6.066  | perplexity:  431.09\n",
            "epoch:  0  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  213.57  | loss:  5.999  | perplexity:  402.98\n",
            "epoch:  0  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  214.19  | loss:  5.913  | perplexity:  369.9\n",
            "epoch:  0  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  220.88  | loss:  5.905  | perplexity:  366.79\n",
            "epoch:  0  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  220.13  | loss:  5.924  | perplexity:  373.72\n",
            "epoch:  0  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  225.43  | loss:  5.805  | perplexity:  332.12\n",
            "epoch:  0  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  218.92  | loss:  5.789  | perplexity:  326.78\n",
            "epoch:  0  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  225.19  | loss:  5.774  | perplexity:  321.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "end epoch:  0 | time:  219.9 s | valid loss:  6.335 | valid ppl:  564.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:  1  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  233.75  | loss:  5.871  | perplexity:  354.61\n",
            "epoch:  1  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  219.93  | loss:  5.707  | perplexity:  301.0\n",
            "epoch:  1  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  225.7  | loss:  5.718  | perplexity:  304.38\n",
            "epoch:  1  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  232.31  | loss:  5.714  | perplexity:  303.02\n",
            "epoch:  1  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  215.14  | loss:  5.69  | perplexity:  295.79\n",
            "epoch:  1  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  225.42  | loss:  5.644  | perplexity:  282.73\n",
            "epoch:  1  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  215.83  | loss:  5.631  | perplexity:  278.83\n",
            "epoch:  1  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  215.23  | loss:  5.58  | perplexity:  264.95\n",
            "epoch:  1  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  221.07  | loss:  5.606  | perplexity:  271.98\n",
            "epoch:  1  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  220.36  | loss:  5.653  | perplexity:  285.18\n",
            "epoch:  1  | batches:  551 / 657  | learning_rate:  0.001 | ms/batch:  225.6  | loss:  5.55  | perplexity:  257.24\n",
            "epoch:  1  | batches:  601 / 657  | learning_rate:  0.001 | ms/batch:  219.15  | loss:  5.56  | perplexity:  259.77\n",
            "epoch:  1  | batches:  651 / 657  | learning_rate:  0.001 | ms/batch:  225.38  | loss:  5.551  | perplexity:  257.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "\n",
            "end epoch:  1 | time:  220.2 s | valid loss:  5.686 | valid ppl:  294.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "epoch:  2  | batches:  51 / 657  | learning_rate:  0.001 | ms/batch:  234.2  | loss:  5.637  | perplexity:  280.54\n",
            "epoch:  2  | batches:  101 / 657  | learning_rate:  0.001 | ms/batch:  219.77  | loss:  5.501  | perplexity:  244.95\n",
            "epoch:  2  | batches:  151 / 657  | learning_rate:  0.001 | ms/batch:  226.52  | loss:  5.515  | perplexity:  248.48\n",
            "epoch:  2  | batches:  201 / 657  | learning_rate:  0.001 | ms/batch:  232.26  | loss:  5.529  | perplexity:  251.81\n",
            "epoch:  2  | batches:  251 / 657  | learning_rate:  0.001 | ms/batch:  216.81  | loss:  5.518  | perplexity:  249.22\n",
            "epoch:  2  | batches:  301 / 657  | learning_rate:  0.001 | ms/batch:  225.01  | loss:  5.479  | perplexity:  239.65\n",
            "epoch:  2  | batches:  351 / 657  | learning_rate:  0.001 | ms/batch:  215.24  | loss:  5.486  | perplexity:  241.4\n",
            "epoch:  2  | batches:  401 / 657  | learning_rate:  0.001 | ms/batch:  215.74  | loss:  5.435  | perplexity:  229.19\n",
            "epoch:  2  | batches:  451 / 657  | learning_rate:  0.001 | ms/batch:  229.25  | loss:  5.47  | perplexity:  237.39\n",
            "epoch:  2  | batches:  501 / 657  | learning_rate:  0.001 | ms/batch:  229.59  | loss:  5.521  | perplexity:  249.88\n",
            "Exiting from training early\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4b8012c62ce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-4b8012c62ce7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------------------------------------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'End of training\\ntest loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ntest ppl: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d00138ee2b71>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(data_source, model)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m              \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-202ec1937f05>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-248d9f735281>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, prev_state)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    save = 'model_test.pt'\n",
        "    torch.manual_seed(1111)\n",
        "    corpus = Corpus()\n",
        "    print(\"len ditc: \", corpus.len_dict)\n",
        "    model = RNN(corpus.len_dict).to(device)\n",
        "    print(\"Dictionary's length: \", corpus.len_dict, \" words.\")\n",
        "\n",
        "    train_data = corpus.train\n",
        "    test_data = corpus.test\n",
        "    val_data = corpus.valid\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99))\n",
        "    # opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    best_val_loss = None\n",
        "\n",
        "    try:\n",
        "        for epoch in range(0, epochs):\n",
        "            epoch_start_time = time.time()\n",
        "            train(model, train_data, opt, epoch)\n",
        "            val_loss = evaluate(val_data, model)\n",
        "            ppl = round(math.exp(val_loss),2)\n",
        "\n",
        "            print(\"-----------------------------------------------------------------------------------------\")\n",
        "            print('end epoch: ', epoch, '| time: ', round((time.time() - epoch_start_time),2), 's | valid loss: ', round(val_loss.item(),3),\n",
        "                  '| valid ppl: ', ppl)\n",
        "            print(\"-----------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "            if not best_val_loss or val_loss < best_val_loss:\n",
        "                with open(save, 'wb') as f:\n",
        "                    torch.save(model, f)\n",
        "                best_val_loss = val_loss\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting from training early')\n",
        "\n",
        "    with open(save, 'rb') as f:\n",
        "        model = torch.load(f)\n",
        "\n",
        "    test_loss = evaluate(test_data,model)\n",
        "    print(\"-----------------------------------------------------------------------------------------\")\n",
        "    print('End of training\\ntest loss: ', round(test_loss.item(),3), '\\ntest ppl: ', math.exp(test_loss))\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jVAJ9T-wCqEd"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "223727_NLU_LM2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}